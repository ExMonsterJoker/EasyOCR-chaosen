# OCR Training Configuration
experiment_name: "modern_ocr_experiment"
seed: 42
saved_model: "saved_models/modern_ocr_experiment"  # Path to pretrained model if any, empty if you want to train from scratch

# Data Configuration
data:
  train_data: "all_data/train_data"  # Root directory containing your dataset folders
  valid_data: "all_data/valid_data"  # Validation data directory
  select_data: ["dataset1"]  # List of dataset folder names under train_data
  batch_ratio: [1.0]  # Ratio for each dataset in select_data
  total_data_usage_ratio: 0.5  # Fraction of total data to use

  # Image preprocessing
  imgH: 48  # Image height
  imgW: 600  # Image width
  rgb: true  # Use RGB images (false for grayscale)
  PAD: true  # Keep aspect ratio with padding

  # Text processing
  character_list: "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~⅛⅜½⅝¾⅞"
  batch_max_length: 300  # Maximum text length
  data_filtering_off: true  # Whether to disable character filtering
  sensitive: true  # Case sensitive training

  # Data loading
  workers: 4  # Number of data loading workers
  augmentation: false  # Enable data augmentation

# Model Configuration
model:
  # Transformation stage
  Transformation:
    name: "None"  # Options: "None", "TPS"
    num_fiducial: 20  # Number of fiducial points for TPS

  # Feature extraction stage
  FeatureExtraction:
    name: "ResNet"  # Options: "VGG", "ResNet"
    output_channel: 512  # Feature channel dimension

  # Sequence modeling stage
  SequenceModeling:
    name: "BiLSTM"  # Options: "None", "BiLSTM"
    hidden_size: 256  # Hidden size for LSTM

  # Prediction stage
  Prediction:
    name: "Attn"  # Options: "CTC", "Attn"

# Training Configuration
training:
  batch_size: 32
  num_iter: 300  # Total training iterations
  valInterval: 100  # Validation interval
  amp: true  # Use Automatic Mixed Precision
  grad_clip: 5.0  # Gradient clipping value
  fine_tune: false  # Fine-tuning mode

  # Optimizer settings
  optimizer:
    type: "adamw"  # Options: "adam", "adadelta", "adamw
    lr: 0.001  # Learning rate
    beta1: 0.9  # Adam beta1
    rho: 0.95  # Adadelta rho
    eps: 1e-8  # Optimizer epsilon

  # Scheduler settings
  scheduler:
    use: true
    type: "OneCycleLR"  # Options: "StepLR", "CosineAnnealingLR", "OneCycleLR"
    step_size: 100000  # Step size for StepLR
    gamma: 0.1  # Gamma for StepLR
    pct_start: 0.3